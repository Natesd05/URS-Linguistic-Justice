{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natesd05/URS-Linguistic-Justice/blob/main/fastText_File_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "loSGICtZ8PNd",
        "outputId": "cb9a7b0e-8158-4227-8cf3-bc3503f6a841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199773 sha256=a50043e2001ee3abb4f655e4bf6ab14905ad5fb21a1c8192a45499e1c760bd24\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.4)\n",
            "Installing collected packages: cloudpathlib, weasel, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpathlib-0.16.0 spacy-3.7.2 weasel-0.3.4\n",
            "Collecting lingua-language-detector\n",
            "  Downloading lingua_language_detector-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (74.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lingua-language-detector\n",
            "Successfully installed lingua-language-detector-2.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext\n",
        "!pip install -U spacy\n",
        "import fasttext\n",
        "import re\n",
        "!pip install lingua-language-detector\n",
        "from lingua import Language, LanguageDetectorBuilder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B37381g68URD",
        "outputId": "4a1931f6-68c3-4c08-8e18-90c9bf28b006"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "/lid.176.ftz cannot be opened for loading!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5c9441c7e628>\u001b[0m in \u001b[0;36m<cell line: 158>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Percentage all spanish: {((self.all_spanish_count/self.total_sentence_count)*100):.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0mLANGUAGE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageIdentification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/clean_sentences.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;31m#LANGUAGE.lang_printout()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;31m#LANGUAGE.print_dictionary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-5c9441c7e628>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpretrained_lang_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/lid.176.ftz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_lang_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspanish_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;34m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0meprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, args)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: /lid.176.ftz cannot be opened for loading!"
          ]
        }
      ],
      "source": [
        "class LanguageIdentification:\n",
        "\n",
        "    def __init__(self, file_path):\n",
        "        pretrained_lang_model = \"/lid.176.ftz\"\n",
        "        self.model = fasttext.load_model(pretrained_lang_model)\n",
        "        self.output = {}\n",
        "        self.spanish_words = 0\n",
        "        self.english_words = 0\n",
        "        self.total_words = 0\n",
        "        self.word_bank = \"\"\n",
        "        self.no_spanish = []\n",
        "        self.little_spanish = []\n",
        "        self.most_spanish = []\n",
        "        self.all_spanish = []\n",
        "        self.no_spanish_count = 0\n",
        "        self.little_spanish_count = 0\n",
        "        self.most_spanish_count = 0\n",
        "        self.all_spanish_count = 0\n",
        "        self.total_sentence_count = 0\n",
        "\n",
        "        with open(file_path, 'r') as file:\n",
        "          self.word_bank = file.read()\n",
        "\n",
        "    def predict_lang(self, text):\n",
        "        text = text.replace('\\n', ' ').strip()\n",
        "        predictions = self.model.predict(text, k=5)\n",
        "        return predictions\n",
        "\n",
        "    def lang_printout(self):\n",
        "        word_list = self.word_bank.split('\\n')\n",
        "\n",
        "        print(\"{:<5} {:<75} {:<10} {:<10}\".format(\"Index\", \"Sentence\", \"Language\", \"Probability\"))\n",
        "\n",
        "        index = 0;\n",
        "\n",
        "        for sentence in word_list:\n",
        "            sentence = sentence.strip()\n",
        "            if sentence:\n",
        "              index += 1\n",
        "              lang = self.predict_lang(sentence)\n",
        "              self.total_words += 1\n",
        "              filtered_lang = [(label, score) for label, score in zip(lang[0], lang[1]) if label in ['__label__en', '__label__es']]\n",
        "              self.output[sentence] = filtered_lang\n",
        "\n",
        "              if filtered_lang:\n",
        "                  highest_prob_label = max(filtered_lang, key=lambda x: x[1])[0]\n",
        "                  highest_prob_score = max(filtered_lang, key=lambda x: x[1])\n",
        "                  if highest_prob_score[1] is not None and highest_prob_score[1] > 0.45:\n",
        "                    if highest_prob_label == '__label__en':\n",
        "                        self.english_words += 1\n",
        "                        language = 'en'\n",
        "                    elif highest_prob_label == '__label__es':\n",
        "                        self.spanish_words += 1\n",
        "                        language = 'es'\n",
        "                    output_line = \"{:<5} {:<75} {:<10} {:<10.4f}\".format(index, sentence, language, highest_prob_score[1])\n",
        "                    print(output_line)\n",
        "              else:\n",
        "                  output_line = \"{:<5} {:<75} {:<10}\".format(index, sentence, \"N/A\")\n",
        "                  print(output_line)\n",
        "\n",
        "    def sentence_sorter(self):\n",
        "        word_list = self.word_bank.split('\\n')\n",
        "\n",
        "        for sentence in word_list:\n",
        "            sentence = sentence.strip()\n",
        "\n",
        "            if sentence:\n",
        "                self.sentence_word_count = 0\n",
        "                self.spanish_count = 0\n",
        "                for word in sentence.split():  # Split sentence into words\n",
        "                    languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH]\n",
        "                    detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
        "                    if detector.detect_language_of(word) == Language.SPANISH:\n",
        "                        self.spanish_count += 1\n",
        "                    self.sentence_word_count += 1\n",
        "\n",
        "                self.total_sentence_count += 1\n",
        "                if (self.spanish_count == 0):\n",
        "                    self.no_spanish_count += 1\n",
        "                    self.no_spanish.append(sentence + '\\n')\n",
        "                    with open('/no_spanish.txt', 'a') as file:\n",
        "                      file.write(sentence + '\\n')\n",
        "                elif (self.spanish_count >= 1 and self.spanish_count <= 3):\n",
        "                    self.little_spanish_count += 1\n",
        "                    self.little_spanish.append(sentence + '\\n')\n",
        "                    with open('/little_spanish.txt', 'a') as file:\n",
        "                      file.write(sentence + '\\n')\n",
        "                elif (self.spanish_count > 3 and self.spanish_count < self.sentence_word_count):\n",
        "                    self.most_spanish_count += 1\n",
        "                    self.most_spanish.append(sentence + '\\n')\n",
        "                    with open('/most_spanish.txt', 'a') as file:\n",
        "                      file.write(sentence + '\\n')\n",
        "                elif (self.spanish_count == self.sentence_word_count):\n",
        "                    self.all_spanish_count += 1\n",
        "                    self.all_spanish.append(sentence + '\\n')\n",
        "                    with open('/all_spanish.txt', 'a') as file:\n",
        "                      file.write(sentence + '\\n')\n",
        "\n",
        "                self.spanish_count = 0\n",
        "                self.sentence_word_count = 0\n",
        "\n",
        "    def print_dictionary(self):\n",
        "      print(self.output)\n",
        "\n",
        "    def print_spanish(self):\n",
        "      print(self.spanish_words)\n",
        "\n",
        "    def print_english(self):\n",
        "      print(self.english_words)\n",
        "\n",
        "    def percentage_spanish(self):\n",
        "      print(self.spanish_words/self.total_words)\n",
        "\n",
        "    def print_total_words(self):\n",
        "      print(self.total_words)\n",
        "\n",
        "    def print_no_spanish(self):\n",
        "      for sentence in self.no_spanish:\n",
        "        print(sentence)\n",
        "\n",
        "    def print_little_spanish(self):\n",
        "      for sentence in self.little_spanish:\n",
        "        print(sentence)\n",
        "\n",
        "    def print_most_spanish(self):\n",
        "      for sentence in self.most_spanish:\n",
        "        print(sentence)\n",
        "\n",
        "    def print_all_spanish(self):\n",
        "      for sentence in self.all_spanish:\n",
        "        print(sentence)\n",
        "\n",
        "    def print_categories_counts(self):\n",
        "      print(f\"No Spanish: {self.no_spanish_count}\")\n",
        "      print(f\"Little Spanish: {self.little_spanish_count}\")\n",
        "      print(f\"Most Spanish: {self.most_spanish_count}\")\n",
        "      print(f\"All Spanish: {self.all_spanish_count}\")\n",
        "\n",
        "    def print_spanish_sentence_percentage(self):\n",
        "        percentage = (self.all_spanish_count / self.total_sentence_count) * 100\n",
        "        print(f\"Percentage of sentences in Spanish: {percentage:.2f}%\")\n",
        "\n",
        "    def print_total_sentence_count(self):\n",
        "        print(f\"Total number of sentences: {self.total_sentence_count}\")\n",
        "\n",
        "    def print_percentage_no(self):\n",
        "        print(f\"Percentage no spanish: {((self.no_spanish_count/self.total_sentence_count)*100):.2f}%\")\n",
        "\n",
        "    def print_percentage_little(self):\n",
        "        print(f\"Percentage little spanish: {((self.little_spanish_count/self.total_sentence_count)*100):.2f}%\")\n",
        "\n",
        "    def print_percentage_most(self):\n",
        "        print(f\"Percentage most spanish: {((self.most_spanish_count/self.total_sentence_count)*100):.2f}%\")\n",
        "\n",
        "    def print_percentage_all(self):\n",
        "        print(f\"Percentage all spanish: {((self.all_spanish_count/self.total_sentence_count)*100):.2f}%\")\n",
        "\n",
        "LANGUAGE = LanguageIdentification(\"/clean_sentences.txt\")\n",
        "#LANGUAGE.lang_printout()\n",
        "#LANGUAGE.print_dictionary()\n",
        "#LANGUAGE.print_spanish()\n",
        "#LANGUAGE.print_english()\n",
        "#LANGUAGE.percentage_spanish()\n",
        "#LANGUAGE.print_total_words()\n",
        "LANGUAGE.sentence_sorter()\n",
        "LANGUAGE.print_most_spanish()\n",
        "LANGUAGE.print_categories_counts()\n",
        "LANGUAGE.print_spanish_sentence_percentage()\n",
        "LANGUAGE.print_total_sentence_count()\n",
        "LANGUAGE.print_percentage_no()\n",
        "LANGUAGE.print_percentage_little()\n",
        "LANGUAGE.print_percentage_most()\n",
        "LANGUAGE.print_percentage_all()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqsnNgXWdMH0KTA8ldzcDq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}